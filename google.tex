\documentclass[12pt, a4paper]{llncs}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage[utf8]{inputenc}
\usepackage[ngerman]{babel}
\usepackage[backend=biber,style=numeric]{biblatex}
\usepackage{csquotes}
\bibliography{bibliography}
\title{Google: Suchalgorithmus und Privacy-Probleme}
\author{Thomas Samy Dafir, Dominik Baumgartner}
\institute{Wissenschaftliche Arbeitstechniken und Präsentation,\newline
	Fachbereich Computerwissenschaften,\newline Universität Salzburg}
\date{\selectlanguage{german}\today}
\hfuzz=8.0pt


\begin{document}
	\maketitle
	\begin{abstract}
	Dieses Paper gibt im ersten Teil einen Einblick in die Funktionsweise der Google-Web-Suchmaschine und die 
	damit verbundenen Technologien, Techniken und Datenstrukturen. Im Rahmen dessen wird genauer auf Web-Crawling,
	Indexierung von Webseiten und den Google PageRank eingegangen. Anhand dieser Komponenten wird dann gezeigt, wie
	eine Google-Suche vom eingeben der Suchbegriffe bis hin zur Ausgabe der Ergebnisse abläuft.
	Im zweiten Teil werden diverse beliebte Google-Dienste wie Google Chrome, Google-Mail oder die Google-Suche
	kritisch hinterfragt und die damit verbundenen Privacy-Probleme erläutert.
	\end{abstract}
	\begin{small}
		\begin{keywords}
		Google, Web-Crawling, Algorithmus, PageRank, Indexierung, Privacy, Google Mail, Google Chrome
		\end{keywords}
	\end{small}
	
	\section{Einführung}
	Web-Suchmaschinen erfreuen sich seit Jahren einer sehr großen Beliebtheit und sind aus unserem heutigen (Online-)Leben kaum 
	mehr wegzudenken. Unter diesen Suchmaschinen hat sich die Google-Suche seit ihrer Einführung im Jahr 1998 zum 
	unumstrittenen Marktführer entwickelt. Dieses Paper geht im ersten Teil grundsätzlich auf die Funktion oder 
	Anatomie der ursprünglichen Version der Google-Suchmaschine und deren Komponenten ein. Nennenswert sind hierbei 
	der Web-Crawler, die Indexierung und der PageRank. Es wird auch gezeigt, welche Techniken Google von damals	etablierten
	Konkurrenten wie Yahoo, Lycos oder AltaVista abhoben und zum Erfolg verhalfen. Zusätzlich wird gezeigt, wie durch
	das Zusammenspiel aller erläuterten Komponenten ein Google-Suchergebnis erzeugt wird. Sämtliche Erläuterungen in
	diesem Teil des Papers beziehen sich auf die ursprüngliche 1998 veröffentlichter Version der Google-Suche, außerdem 
	sind viele Zusammenhänge vereinfacht dargestellt.\newline
	Google ist schon seit einiger Zeit nicht mehr nur eine Suchmaschine, sondern stellt ein ganzes Online-Ökosystem an
	miteinander verknüpften Diensten zur Verfügung, die sich bei Usern aufgrund der ständigen Erreichbarkeit immer 
	wachsender Beliebtheit erfreuen. Mit dieser steigenden Menge an Diensten des selben Unternehmens steigt natürlich 
	auch die Menge an persönlichen Benutzerdaten, über die das Unternehmen verfügt. Damit stellt sich immer öfter die Frage,
	wie mit diesen Daten umgegangen wird. Damit beschäftigt sich der zweite Teil dieses Papers. Wir zeigen anhand 
	einiger Beispiele, dass Datenschutz und Privatsphäre im Google-Universum keineswegs allerhöchste Priorität genießen.
	
	\section{Web-Crawling}\label{sec:google}
	\subsection{Grundlagen}
	Web-Crawler sind Programme, die das Web durchforsten und in den verschiedensten Anwengungsgebieten zum auffinden von Daten
	verwendet werden. Unter anderem verwenden viele Web-Suchmaschinen, wie auch Google, Web-Crawler, um Daten
	über Webseiten zu sammeln. Dies ergibt neben dem Informationsgewinn für den Initiator des Crawlers auch einen
	Mehrwert für die gecrawlte Webseite, da diese dann im Index des jeweiligen Suchanbieters, in unserem Fall Google,
	aufscheint\cite{giles10}.
	Das Erstellen und Betreiben eines Web-Crawlers ist eine schwierige Aufgabe, da sehr viele Performance- und Verlässlichkeits-
	Aspekte berücksichtigt werden müssen und da ein Crawler mit einer riesigen Anzahl von Web- und DNS-Servern interagieren
	muss\cite{page98}.
	Aufgrund der Größe des World-Wide-Web und dessen immer weitere Expansion, sowie der Tatsache, dass Webseiten, die
	einmal gefunden wurden immer wieder verändert werden, müssen Crawler riesige Datenmengen verarbeiten und eine große
	Menge von Webseiten pro Sekunde crawlen können, um den Suchindex einer Suchmaschine aktuell halten zu können\cite{hafri04}.
	Die bereits erwähnte Expansion und Änderung von Webseiten führt dazu, dass das zu crawlende Web eine praktisch unendliche
	Größe erreicht. Ein mit dem Ziel alle Webseiten des WWW zu crawlen und damit einen aktuellen Index des gesamten Web zu erstellen
	gestarteter Crawler wird dieses Ziel nie erreichen können, da aufgrund der ständigen Veränderung des Web bereits gecrawlte
	Seiten immer wieder neu gecrawlt werden müssen, um einen aktuellen Index-Eintrag zu gewährleisten\cite{yates05}.
	
	\subsection{Google's Web-Crawler}
	Google verwendet Web-Crawler in einem verteilten System. Die zentralen Aufgaben wurden aufgeteilt. Ein zentraler URL-Server
	verteilt URLs auf einige parallel laufende Crawler. Diese Crawler nehmen die URLs vom URL-Server entgegen, verfolgen sie,
	laden die damit verbundenen HTML-Dokumente herunter und senden diese an einen Store-Server, der die Dokumente in einem so
	genannten Repository zur späteren Analyse (Parsing) und Indexierung ablegt (siehe Kapitel \ref{indexDat}: Indexierung und Datenstrukturen).
	Beim Parsen werden URLs aus den Dokumenten extrahiert und wiederum dem URL-Server zur späteren Verteilung auf die Crawler
	zugeführt (Extrahieren der URLs kann einfach durchgeführt werden, da diese im HTML-Dokument einfach anhand von Tags zu identifizieren
	sind).
	Um eine Webseite zu erreichen müsste jeder Crawler zuerst den jeweiligen zur URL gehörenden DNS-Server nach der IP der Seite befragen.
	Da sich DNS in verschiedenen Staaten oder auf verschiedenen Kontinenten befinden, können dadurch aufgrund der variierenden Verbindungsgeschwindigkeit enorme
	Einbußen bei der Crawl-Performance zustande kommen.
	Um das zu verhindern und die Effizienz und Performance zu erhöhen, hat jeder Crawler seinen eigenen DNS-Cache, um nicht für jede zu verfolgende
	URL zuerst eine Abfrage	am jeweiligen DNS-Server durchführen zu müssen\cite{page98}.
	
	
	
	\section{Indexierung und Datenstrukturen}\label{indexDat}
	\subsection{Allgemein}
	Parsing, wie im vorherigen Kapitel beschrieben, wird nicht nur für die URLs und damit für das Crawling durchgeführt, sondern ist auch Teil
	des Indexierungsprozesses. Grundsätzlich gilt, dass die von den Crawlern heruntergeladenen Webseiten natürlich nicht unkomprimiert gespeichert
	werden. Die Indexierung beginnt immer damit, dass wichtige Informationen aus den Dokumenten extrahiert werden. Diese Informationen beinhalten
	sämtliche als wichtig erkennbare Wörter und Textstellen des Dokuments: Titel der Webseite, Hyperlinks oder hervorgehobener Text (z.B. fett, kursiv).
	Diese extrahierten Informationen werden dann verwendet, um Indexeinträge für die jeweilige Webseite zu erstellen und diese damit über die Suchmaschine
	auffindbar zu machen\cite{langville11}.\newline
	Im nächsten Kapitel wird genauer auf Indexstrukturen der Google Suchmaschine eingegangen.
	
	\subsection{Google}
	
	
	\subsubsection{Repository}
	Im Repository werden die, von den Crawlern heruntergeladenen Webseiten als HTML-Quelltext gespeichert und komprimiert. Bereits hier wird jedem Dokument
	eine DocID zugewiesen, um es eindeutig identifizieren zu können. Außerdem wird zu jedem Dokument die Größe und URL gespeichert\cite{page98}.

	\subsubsection{Document Index}
	Der Document Index enthält essenzielle Information zu jeder Webseite. Die Einträge sind nach der DocID der Dokumente sortiert und enthalten jeweils einen
	Pointer auf den komprimierten Quelltext im Repository, die Checksumme zum jeweiligen Dokument sowie diverse Statistiken. Zusätzlich existiert eine Datei, in
	der zu jeder URL die DocID abgelegt ist. Die Datei ist nach den Checksummen der URLs sortiert. Diese Datei wird im weiteren verwendet, um zu einer URL die
	jeweilige DocID zu finden. Dazu wird die Checksumme der URL berechnet und binary-search auf diese Datei ausgeführt\cite{page98}.
	
	\subsubsection{Hit Lists}
	Für jedes Dokument werden Hit Listen erstellt die die Häufigkeit und Art des Vorkommens aller Wörter enthalten. Da diese Hitlisten sehr viel Speicher einnehmen,
	wurde eine eigene, effiziente und kompakte Kodierung entwickelt. Somit wird jeder Hit durch 2 Byte repräsentiert. Ein Hit List enthält zu jedem auftreten eines Wortes
	Informationen, wie etwa in welcher Schriftgröße es vorkommt, ob es in einer Überschrift, im Titel, in einem meta-tag oder als Hyperlink vorkommt und an welcher Stelle
	im Dokument das Wort gefunden wurde. Die Länge einer Hit List, also die Häufigkeit des Vorkommens eines Begriffs in einem Dokument ist im Head jeder Hit List abgelegt\cite{page98}.
	
	\subsubsection{Forward Index}
	Der Forward Index ist auf 64 sogenannte Barrels aufgeteilt. Die Einträge in diesem Index sind nach WordIDs aufgeteilt. Jedes Wort hat eine eindeutige WordID und ist 
	einem bestimmten Barrel zugewiesen. Enthält jetzt ein Document ein bestimmtes Wort, wird eine neuer Eintrag im Barrel erstellt, dem das Wort zugewiesen ist. Dieser
	Eintrag besteht aus einem Head-Teil, der die DocID des Dokuments enthält und der jeweiligen WordID. Außerdem enthält jeder Eintrag auch die Hit List für das jeweilige
	Wort im jeweiligen Dokument. Ein derartiger Eintrag wird für jedes Wort in jedem Dokument im passenden Barrel erstellt. Zu erwähnen ist auch, dass durch diese Aufteilung
	eine teilweise Sortierung der Einträge erreicht wird, da jedes Barrel nur einen Bereich von Wörtern enthält. Diese Sortierung führt zu beschleunigten Abfragen, da für eine
	Anfrage nur ein Teil der Einträge durchsucht werden muss\cite{page98}.
	
	\subsubsection{Lexikon und Inverted Index}
	Der Inverted Index besteht aus den selben Barrels wie der Forward Index (die gleichen Daten). Die Einträge sind hier allerdings nach der DocID sortiert.
	Das Lexikon enthält alle Wörter, nach denen gesucht werden kann. Jeder Eintrag im Lexikon besteht aus einer eindeutigen WordID und einer Liste von Pointern, die auf den
	Eintrag im jeweiligen Barrel zeigen, der das Wort enthält. Jeder Eintrag im Lexikon zeigt also auf eine Liste von DocIDs; dies sind alle Dokumente (Webseiten), die das
	Wort mit der gespeicherten WordID enthalten. Zu jedem dieser Einträge (DocIDs) enthalten die Inverted Barrels die zugehörige Hit List, die angibt, wie oft und in welcher
	Qualität das Wort im Dokument vorkommt\cite{page98}.
	
	
	\section{PageRank}
	Ende der 90er Jahre wurde noch von keiner einzigen Suchmaschine eine Methode zur Gewichtung der Suchergebnisse nach ihrer Wichtigkeit oder Popularität verwendet.
	Sämtliche damals etablierte Suchmaschinen verwendeten ein Ranking-System, das nur inhaltliche Analysen berücksichtigte, also Resultate praktisch nur nach der Häufigkeit
	des Vorkommens des Suchbegriffs oder dem letzten Änderungsdatum bewertete. Dieses System liefert nur unzureichende Ergebnisse und kann zusätzlich von Webseitenbetreibern 
	leicht manipuliert werden (beispielsweise indem bestimmte Suchbegriffe sehr oft in versteckte Felder geschrieben werden).\cite{wills06}.
	Aufgrund dessen wurden 1998 beinahe gleichzeitig zwei Projekte mit dem Ziel	gestartet, Suchresultate zu verbessern und ein Wichtigkeits-Maß für Webseiten zu erstellen.
	Eines dieser Projekte mit Namen PageRank wurde von den Google Gründern Sergey Brin und Lawrence Page initiiert und das daraus resultierende Ergebnis ist bis heute
	der Kern der Google-Websuche\cite{langville11}.
	Das PageRank Prinzip war bis zur Entwicklung durch Brin und Page unbekannt und wurde auch 1998 als Patent auf Lawrence Page eingereicht und 2001 akzeptiert\cite{google98}.

	\subsection{Allgemein}
	Das Web kann als gerichteter Graph interpretiert werden. Dabei repräsentieren Knoten Webseiten und Kanten Hyperlinks zwischen Webseiten. PageRank verwendet
	diese Struktur, um eine Aussage über die Wichtigkeit einer Webseite zu treffen und somit den PageRank zu berechnen. Die Wichtigkeit einer Webseite ist von drei
	Faktoren abhängig: erstens von der Anzahl der Links, die von anderen Seiten auf die aktuelle Seite zeigen, zweitens von der Gesamtanzahl der Links auf jeder
	verlinkenden Seite und drittens vom PageRank der jeweiligen verlinkenden Seite. Jeder Link auf die aktuelle Seite kann als eine Art Empfehlung verstanden werden
	und trägt damit zum PageRank bei. Natürlich ist auch wichtig zu erfassen, wie wichtig die Seite ist, die auf die aktuelle Seite verlinkt: je wichtiger die
	verlinkende Seite ist (also je höher ihr PageRank), desto wichtiger muss demnach auch die Seite sein, auf die verlinkt wird. Betrachtet man einen Link als
	Empfehlung wird auch das dritte Kriterium schnell klar: je mehr Links eine Webseite enthält (also je freigiebiger eine Webseite ist), desto geringer ist die
	Relevanz eines einzelnen Links\cite{langville11}.
	Der PageRank kann auch aus dem Modell eines "Random Surfers" heraus interpretiert werden. Man betrachte eine Person, die eine zufällige Webseite als Startpunkt
	bekommt. Diese Person bewegt sich nun durchs Web, indem Sie eine Folge von Hyperlinks klickt und sich dabei immer nur vorwärts bewegt, also nie zurück auswählt.
	Der PageRank einer Webseite ist jetzt die Wahrscheinlichkeit vom Random Surfer auf seiner Reise durchs Web besucht zu werden\cite{page98}.
	
	\subsection{Mathematischer Hintergrund}
	Die ursprüngliche Art und Weise den PageRank zu berechnen, basiert auf folgender Summationsformel:
	\begin{displaymath}
		r(P_i)=\sum_{P_j\in B_{P_i}} \frac{r(P_j)}{|P_j|}
	\end{displaymath}
	Die Funktion $r(P_x)$ repräsentiert den PageRank einer Webseite $P_x$.\newline
	$|P_x|$ repräsentiert die Anzahl der Links auf der Seite $P_x$.\newline
	$B_{P_i}$ ist die Menge aller Webseiten, die auf die Seite $P_i$ linken.\newline
	Um den PageRank einer Seite $P_i$ zu berechnen, wird für jede Webseite $P_j$, die auf die Seite $P_i$ linkt der Quotient des PageRank der jeweiligen Seite zur
	Gesamtanzahl der Links auf der Seite $P_j$ gebildet. Die Aufsummierung all dieser Quotienten ergibt den PageRank der Webseite $P_i$. Je mehr Webseiten also auf
	eine Seite linken, je größer der PageRank der linkenden Seiten und je weniger Links die verlinkenden Seiten enthalten, desto größer ist der PageRank dieser
	Seite und damit die Wahrscheinlichkeit, vom Random Surfer erreicht zu werden.\newline
	In der Realität ist der PageRank aller anderen Seiten noch nicht bekannt. Deshalb wird ein iterativer Prozess angewandt. Anfangs wird allen Webseiten der gleiche
	PageRank (z.B. $1/n$ wobei $n$ die Anzahl der Webseiten im Google-Index ist) zugewiesen. Ausgehend von diesen Ausgangswerten werden Iterationen anhand nachfolgender
	Formel durchgeführt. In jeder Iteration wird auf das Ergebnis der vorherigen Iteration zurückgegriffen und somit das Ergebnis immer weiter verfeinert, bis ein stabiler
	Wert erreicht ist\cite{langville11}.
	\begin{displaymath}
	r_{k+1}(P_i)=\sum_{P_j\in B_{P_i}} \frac{r_k(P_j)}{|P_j|}
	\end{displaymath}
	
	
	\section{Ablauf der Suche}
	In den vergangenen Kapiteln haben wir uns damit befasst, aus welchen Komponenten die Google Web-Suchmaschine besteht und was sie von der Konkurrenz unterscheidet.
	Nun werfen wir einen Blick darauf, wie all diese Komponenten zusammenspielen, um ausgehend von einem eingegebenen Suchbegriff hochqualitative Suchergebnisse zu
	liefern. Page und Brin\cite{page98} beschreiben den Ablauf folgendermaßen:
	\begin{enumerate}
	\item Der Suchvorgang startet damit, dass der eingegebene Suchstring in einzelne Wörter zerlegt wird.
	\item Diese Wörter werden dann zu WordIDs konvertiert, da der Index nur nach WordIDs und nicht nach Wörtern im Klartext durchsucht werden.
	\item Jede WordID wird im Lexikon des Inverse Index gesucht. Es muss also zuerst der Anfang der Dokumentenliste zu jedem Wort im Inverse Index gefunden werden.
	\item Alle Dokumentenlisten werden parallel durchsucht, bis ein Dokument gefunden wird, das alle Suchbegriffe enthält.
	\item Dieses gefundene Dokument wird dann als vorläufiges Ergebnis vorgemerkt und der Rank berechnet. Der Rang ist eine Art Punktezahl, anhand derer
		die gefundenen Ergebnisse am Ende sortiert werden können. Er ergibt sich aus einer Kombination von PageRank und Treffer-Informationen aus den Hit Lists:
	\begin{itemize}
		\item Wird ein Suchbegriff als auf einer Webseite wichtiges Wort erkannt, also ist es beispielsweise fett gedruckt, im Seitentitel in einer Überschrift, oder
			sogar in der URL der Seite enthalten werden für den Treffer mehr Punkte vergeben, als für einen normalen Treffer im Fließtext. Google erzielt auch hier
			einen Qualitätsgewinn gegenüber anderen Web-Suchmaschinen, da sehr viele Statische Werte erfasst werden.
		\item Die Trefferzahl erzielt anfangs einen großen Anstieg der Punktezahl, mit Steigender Anzahl der Treffer, nimmt jedoch der Zuwachs ab\footnote{eventuell
			logarithmische Wachstumsfunktion \textless Anm. d. Verf.\textgreater}.
		\item Bei Sucheingaben, die aus mehreren Wörtern bestehen, werden um so mehr Punkte vergeben, je näher die Suchbegriffe beieinander liegen.
		\item Diese Punkte werden dann mit dem jeweiligen PageRank der Webseite kombiniert und ergeben den Gesamtrang.
	\end{itemize}
	\item Dieser Vorgang wird ab Punkt 4 so lange wiederholt, bis entweder die erforderliche Anzahl an Treffern erreicht wurde, oder bis keine Suchergebnisse mehr gefunden
		werden können.
	\item Abschließend werden dann alle vorgemerkten Suchergebnisse nach ihren Gesamtrang sortiert und die k besten ausgegeben, wobei k eine festgelegte Anzahl von Ergebnissen
		ist\footnote{Derzeit liegt k bei ca. 1000 \textless Anm. d. Verf.\textgreater}.
	\end{enumerate}
	Nach der Abarbeitung all dieser Schritte erhält der Benutzer die nach Relevanz sortierten im Google-Index enthaltenen Webseiten als Ergebnis.
	
	\section{Google Privacy Probleme}

	Die Nutzung des modernen Internets bringt sehr viele Vorteile mit sich, wie z.B. massig Information zu bestimmten Themen, Unterhaltung, oder praktische Dienstleistungen von verschiedenen Anbietern durch diverse Programme oder Webapplikationen. Jedoch bringen manche dieser Produkte neben Vorteilen auch einige Nachteile. Bei den meisten Anbietern muss, bevor dessen Dienstleistung genutzt werden kann eine Registrierung durchgeführt werden, wodurch bereits einige persönliche Daten an den Betreiber übermittelt werden. Dies geschieht jedoch meistens mit Einverständnis des Benutzers. Es gibt aber auch viele Wege diese Daten zu bekommen ohne dass der Benutzer etwas darüber weiß. Vor allem Google benutzt sehr viele Programme die sensible Daten sammeln. Da Google sehr viele Dienstleistungen anbietet, werden wir nur auf einige dieser Programme in den folgenden Abschnitten eingehen.
	
	\subsection{Google Analytics}
	Google Analytics ist eine Dienstleistung von Google für Webseitenbetreiber, mit deren Hilfe die Seite für den angebotenen Dienst optimiert und Informationen über die Nutzung der Webseite gesammelt werden können. Diese Informationen können entweder selbst durch die Auswertung der eigenen Webserver-Protokolle, oder mit Hilfe von sogenannten Web-Tracking-Diensten erlangt werden.
	Wie in \cite{1} beschrieben ist Google Analytics der bekannteste und marktführende Web-Tracking-Dienst, der einem Webseitenbetreiber kostenlos eine relativ detaillierte, grafisch aufbereitete Auswertung der
	Webseiten-Nutzung anbietet. Dieser Dienst sammelt ohne Einwilligung des Benutzers und meistens auch ohne dessen Wissen Daten. Einige dieser Daten sind z.B. die Namen der besuchten Webseiten und die Uhrzeit zu der diese Seiten besucht wurden. D.h. Google Analytics kann die Nutzung und das Verhalten der Benutzer beim Surfen im Internet analysieren und dadurch Statistiken und Übersichten für die Webseitenbetreiber erstellen. Diese Übersichten enthalten Informationen über Benutzer wie beispielsweise von welcher Seite kommend, über welchen Zeitraum welche Seiten besucht wurden.
	Dieser Dienst wird von Google zwar kostenfrei, aber nicht uneigennützig zur Verfügung gestellt, denn durch die Nutzung des Dienstes erhält auch Google Informationen über Nutzer und Webseiten, wie Besucherzahlen und Benutzerbewegung auf Webseiten. Google könnte diese Daten nun verwenden, sie mit Daten von anderen Webseiten abgleichen und somit Informationen über Webseiten-Betreiber, deren Kunden, Angebote und vieles mehr zu sammeln.
	
	\subsection{Google Chrome}
	Google Chrome ist der Internetbrowser des Unternehmens Google. Wie in \cite{2} beschrieben, gibt es auch hier einige Aspekte im Bezug auf die Privatsphäre. In diesem Abschnitt behandeln wir nur drei davon: Die Omnibox, die User-ID und die Cookies. 
	
	\subsubsection{Die Omnibox}
	Die Omnibox ist die Adresszeile des Browsers, die nicht nur zur Eingabe der URL dient, sondern auch zur Vervoll- ständigung der aktuell eingegebenen Zeichen, auch Vorschlagfunktion genannt, dient. Hierbei wird die noch nicht vollständig eingegebene URL nach jedem Tastendruck automatisch an Google gesendet. Diese Datenübertragung erfolgt ohne dass der Benutzer die eingegebene Adresse mit der Tastatur bestätigen muss. Wenn eine nicht existente Adresse eingetippt wird, so versucht Chrome mithilfe der Suchmaschine, die bei der Installation standardmäßig gesetzt wurde (im Normalfall Google), passende Ergebnisse anzuzeigen und weiter zu Helfen. Welche Privacy Probleme bei der Google Suche entstehen werden in \ref{sec:search} genauer beschrieben.

	\subsubsection{Die User-ID}
	\label{User-ID}
	Größtenteils unbekannt ist der Fakt, dass in jeder installierten Kopie von Google Chrome eine sog. Identifikationsnummer oder User-ID versteckt ist. Die User-ID ist eindeutig identifizierbar und wird bei jeder Update Anfrage an Google gesendet. Nun ist es sogar möglich über diese Identifikationsnummer die von Chrome gesetzten Cookies und auch in die Omnibox eingetippten Adressen einem bestimmten Benutzer zuzuordnen. In Kombination mit anderen personalisierten Google-Diensten, wie z.B. Google+, wo personenbezogene Daten zur Erstanmeldung angeben werden müssen, oder mit Google verbundenen Diensten Dritter, ist es möglich diesen bestimmten Benutzer einer namentlich identifizierten Person zuzuordnen.
	
	\subsubsection{Cookies:}
	Cookies sind Informationen, die vom Webserver, wie z.B. eine Anfrage bei der Google Suchmaschine, an den Internet Browser gesendet, dann von diesem gespeichert und bei jedem späteren Aufruf des Webservers an diesen zurückgesendet werden. So kann sich Google bestimmte Einstellungen merken, wie beispielsweise die gewünschte Sprache, in der Suchergebnisse angezeigt werden sollen. Wenn nun der Browser Cookies setzt, enthalten diese eine zufällig generierte ID, die bei jedem setzen des Cookies neu erzeugt wird. Wird nun Chrome gestartet, die Vorschlagfunktion der Omnibox verwendet oder andere Google Dienste aufgerufen, so werden alle für die Domäne google.com gesetzten Cookies zusammen mit der entsprechenden Anfrage direkt zu Google gesendet. Darunter fallen auch Cookies die von anderen Google Diensten, z.B. Google Mail, gesetzt wurden. Dadurch ist es sogar möglich, mit der bei Update Anfrage mitgesendeten User-ID (siehe \ref{User-ID}) diese gesetzten Cookies einer bestimmten Browser-Installation zuzuordnen. Diese Zuordnung ist einfacher durchführbar als eine Identifizierung über die IP-Adresse des Benutzers. Wie auch bei der User-ID, ist es auch hier durch Kombination von anderen Google Diensten oder mit Google verbundenen Diensten Dritter möglich, diese Informationen persönlichen Daten zuzuordnen.
	
	\subsection{Google Mail:}
	Google Mail oder kurz gMail ist der E-Mail Dienst des Unternehmens Google. Wie bei allen anderen Google Diensten ist auch bei gMail die Privatsphäre ein sehr heikles Thema, denn gMail durchsucht Mails automatisiert, wobei gewisse Stichwörter herausgefiltert werden um eine Kontext bezogene Werbung einblenden zu können. Obwohl man beim erstellen eines Zugangs den Nutzungsdaten zustimmen muss ist es aus Datenschutzgründen problematisch, da versendete Mails von Nutzer anderer E-Mail Anbieter, die diesen Bestimmungen nicht zugestimmt haben, trotzdem von Google durchsucht werden. Weiters sind auch die Lizenzbestimmung von gMail problematisch, da sie es nicht nur erlauben E-Mails zu indizieren, was bedeutet dass diese bei einer einfachen Suche in einer Suchmaschine gefunden und von jeder Person eingesehen werden können, sonder auch nicht garantieren, dass vom Benutzer gelöschte E-Mails wirklich gelöscht werden.
	
	\subsection{Die Google Suche:}
	\label{sec:search}
	Die Google Suchmaschine oder Google Search ist die beliebteste Suchmaschine im Internet, unter anderem da sie kostenlos, umfangreich und schnell ist. Genauere Details sind ab Kapitel \ref{sec:google} zu finden. Aber auch Google Search ist aus einer Privatsphärensicht betrachtet nicht optimal. Wird etwa eine Suchanfrage gestartet, setzt Google Cookies. Diese dienen dazu, möglichst viele Daten über den Benutzer zu sammeln, welche dann von Google ausgewertet werden, um ein so genanntes Interessenprofil zu erstellen. Dadurch ist es Google möglich, Werbung möglichst individuell auf den Benutzer abzustimmen und diese immer passend einzublenden. Wenn nun jemand beispielsweise nach einem Regal von Ikea sucht, bekommt diese Person höchst wahrscheinlich Werbungen von verschiedenen Regalen angezeigt\cite{3}.

	\section{Fazit}
	Zusammenfassend, kann gesagt werden, dass das ordnungsgemäße Funktionieren einer Web-Suchmaschine von vielen Komponenten ab- hängig ist. Diese Komponenten
	sind auf die ein oder andere Art und Weise in jeder Suchmaschine enthalten und waren dies auch schon vor der Gründung von Google. Die Technologie, die es
	der Google Suche ermöglichte, sich maßgeblich von der Konkurrenz abzuheben, ist ihre Fähigkeit, wichtige und relevante Suchergebnisse von hoher Qualität zu 
	liefern. Dies wurde vorrangig durch die Erfindung von PageRank ermöglicht. Diese Klassifizierung von Webseiten nach ihrer Wichtigkeit und der damit einhergehende
	Erfolg waren aber nur der Startpunkt der Google Erfolgsgeschichte. Bis heute hat sich dieses Unternehmen, dessen Geschichte mit einer Web-Suchmaschine begann zum Provider einer riesigen Menge an großteils kostenlosen Diensten entwickelt, das jedoch auf dem Gebiet der Datensammlung immer aktiver wird. Wie die vorhergegangenen
	Kapitel zum Thema Privacy-Probleme gezeigt haben, beschränkt sich Google nicht darauf, wie viele andere Unternehmen beispielsweise anonymisierte Nutzungsstatistiken zur 
	Verbesserung der eigenen Dienste zu erheben, sondern zielt bewusst darauf ab, Nutzer zu identifizieren und durch deren Nutzung der eigenen Dienste detaillierte
	Informationen zu erhalten. Meist hat der Nutzer von dieser Datensammlung keine Kenntnis.
	Ohne Zweifel stellt Google sehr viele praktische Tools, unter anderen die hier genauer behandelte Web-Suchmaschine, zur Verfügung. Jedoch sollte sich jeder Nutzer die
	Frage stellen, ob sich die Aufgabe der Privatsphäre durch die Vorteile, die aus der Benutzung dieser Tools erwachsen aufwiegen lässt.
	\newpage	
	\printbibliography[title={Bibliographie}]
	
\end{document}